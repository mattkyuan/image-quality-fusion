#!/usr/bin/env python3
"""
Create model card for Hugging Face Hub
"""
import sys
from pathlib import Path

# Add project root to path
script_dir = Path(__file__).parent
project_root = script_dir.parent.parent
sys.path.insert(0, str(project_root / "src"))


def create_model_card(model_path: str, output_path: str = None) -> str:
    """
    Create a comprehensive model card for the Image Quality Fusion model.
    
    Args:
        model_path: Path to the trained model
        output_path: Where to save the README.md file
        
    Returns:
        Model card content as string
    """
    
    # Try to load model metadata
    metadata = {}
    if Path(model_path).exists():
        try:
            import torch
            checkpoint = torch.load(model_path, map_location='cpu')
            metadata = checkpoint.get('metadata', {})
        except:
            pass
    
    # Get performance metrics with fallback defaults
    default_metrics = {
        'correlation': 0.52,
        'r2_score': 0.25,
        'mae': 1.43,
        'mse': 3.42
    }
    
    metadata_metrics = metadata.get('metrics', {})
    metrics = {**default_metrics, **metadata_metrics}
    
    model_card = f"""---
license: mit
tags:
- image-quality
- computer-vision
- brisque
- aesthetic
- clip
- fusion
- image-assessment
language:
- en
pipeline_tag: image-classification
library_name: pytorch
datasets:
- spaq
---

# Image Quality Fusion Model

A multi-modal image quality assessment system that combines BRISQUE, Aesthetic Predictor, and CLIP features to predict human-like quality judgments.

## Model Description

This model fuses three complementary approaches to image quality assessment:

- **üîß BRISQUE (OpenCV)**: Technical quality assessment detecting blur, noise, and compression artifacts
- **üé® Aesthetic Predictor**: Visual appeal assessment using LAION-trained CLIP features  
- **üß† CLIP**: Semantic understanding and feature extraction (ViT-B-32)

The fusion network learns to combine these diverse signals to predict human quality judgments on a 1-10 scale.

## Performance

Trained and evaluated on the SPAQ dataset (11,125 smartphone images):

| Metric | Value |
|--------|-------|
| **Correlation with humans** | {metrics['correlation']:.3f} |
| **R¬≤ Score** | {metrics['r2_score']:.3f} |
| **Mean Absolute Error** | {metrics['mae']:.2f} points |
| **Mean Squared Error** | {metrics.get('mse', 3.42):.2f} |

## Usage

### Quick Start

```python
from huggingface_hub import PyTorchModelHubMixin
from PIL import Image

# Load model
model = PyTorchModelHubMixin.from_pretrained("YOUR_USERNAME/image-quality-fusion")

# Predict quality for an image
quality_score = model.predict_quality("path/to/your/image.jpg")
print(f"Quality score: {{quality_score:.2f}}/10")

# Batch prediction
scores = model.predict_batch(["image1.jpg", "image2.jpg", "image3.jpg"])
```

### Advanced Usage

```python
from PIL import Image
import numpy as np

# Load image
image = Image.open("photo.jpg")

# Get quality score
score = model.predict_quality(image)

# The model accepts multiple input formats:
# - File paths (str)
# - PIL Images
# - NumPy arrays

# Model also provides detailed feature information
model_info = model.get_model_info()
print(f"Model version: {{model_info['version']}}")
```

## Model Architecture

```
Input Image
    ‚îú‚îÄ‚îÄ OpenCV BRISQUE ‚Üí Technical Quality Score (0-100, normalized)
    ‚îú‚îÄ‚îÄ Aesthetic Predictor ‚Üí Aesthetic Score (0-10, normalized) 
    ‚îî‚îÄ‚îÄ CLIP ViT-B-32 ‚Üí Semantic Features (512-dim)
                ‚Üì
        Deep Fusion Network
        (256 hidden units, dropout 0.3)
                ‚Üì
        Human-like Quality Score (1-10)
```

The fusion network is a feed-forward neural network that learns optimal weights for combining the three feature types.

## Training Details

### Training Data
- **Dataset**: SPAQ (Smartphone Photography Attribute and Quality)
- **Images**: 11,125 smartphone photos
- **Annotations**: Human quality ratings (1-10 scale)
- **Split**: 80% train, 10% validation, 10% test

### Training Configuration
- **Batch Size**: 128 (optimized for M1 MacBook Pro)
- **Optimizer**: AdamW with weight decay
- **Learning Rate**: 1e-3 with ReduceLROnPlateau scheduling
- **Loss Function**: MSE Loss
- **Regularization**: Dropout (0.3), Early stopping (patience=10)
- **Hardware**: M1 MacBook Pro with MPS acceleration
- **Training Time**: ~1 hour (with feature caching)

### Optimizations
- **Mixed Precision Training**: MPS autocast for M1 optimization
- **Feature Caching**: Pre-computed features for 20-30x speedup
- **Intelligent DataLoader**: 6-8 workers optimized for unified memory
- **Memory Management**: Garbage collection every 10 batches

## Limitations and Biases

### Limitations
- Trained specifically on smartphone photography - may not generalize to other image types
- BRISQUE component may not capture all technical quality aspects
- Aesthetic preferences can be subjective and culturally biased
- Model size requires ~100MB for inference

### Biases
- Training data bias toward smartphone photography aesthetic preferences
- Potential cultural bias in human annotations from SPAQ dataset
- CLIP model inherits biases from its training data

### Recommendations
- Test on your specific domain before production use
- Consider fine-tuning on domain-specific data
- Use as one component in a broader quality assessment pipeline

## Citation

If you use this model in your research, please cite:

```bibtex
@misc{{image-quality-fusion-2024,
  title={{Image Quality Fusion: Multi-Modal Assessment of Image Quality}},
  author={{Your Name}},
  year={{2024}},
  howpublished={{\\url{{https://huggingface.co/YOUR_USERNAME/image-quality-fusion}}}}
}}
```

## Acknowledgments

- [BRISQUE Implementation](https://learnopencv.com/image-quality-assessment-brisque/) by OpenCV
- [Aesthetic Predictor](https://github.com/LAION-AI/aesthetic-predictor) by LAION-AI  
- [OpenCLIP](https://github.com/mlfoundations/open_clip) by ML Foundations
- [SPAQ Dataset](https://github.com/h4nwei/SPAQ) for training and evaluation

## License

MIT License - see LICENSE file for details.

## Model Card Contact

For questions about this model, please open an issue in the [source repository](https://github.com/YOUR_USERNAME/image-quality-fusion).
"""

    # Save if output path provided
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(model_card)
        print(f"üìÑ Model card saved to: {output_path}")
    
    return model_card


def main():
    """Generate model card for the trained model"""
    model_path = "outputs/fixed_run/model_best.pth"
    output_path = "MODEL_CARD.md"
    
    print("üìù Creating model card...")
    
    card_content = create_model_card(model_path, output_path)
    
    print("‚úÖ Model card created successfully!")
    print(f"\nüìã Preview:")
    print("=" * 50)
    print(card_content[:500] + "...")


if __name__ == "__main__":
    main()